{
  "articles": [
    {
      "path": "index.html",
      "author": [],
      "contents": "\n\nContents\nIntroduction\nSensitivity to the Choice of Examples\nLearning to Select Examples\nThe Framework\nReward Function\n\nExperiments\nResults\n\nConcluding Discussion\n\nIntroduction\nLarge language models, such as GPT-3 (Brown et al. 2020) demonstrate\nan emergent capability, known as in-context learning, to perform a task by\nsimply observing information (such as instructions and demonstration examples)\nin its prompt. Despite its incredible success on many tasks, in-context learning\nperformance very much depends on a good prompt (Mishra et al. 2022).\nIn this work, we approach prompting from the perspective of example selection.\nThat is, we seek to answer: how to select good examples for in-context learning?\nDifferent from prior work which retrieves examples assuming access to individual\ntest instances (Liu et al. 2022; Rubin, Herzig, and Berant 2022), we\naim to select good examples for the entire testing distribution.\nSensitivity to the Choice of Examples\nSensitivity of in-context learning to changes in the prompt is identified by\nprior work (Zhao et al. 2021; Lu et al. 2022).\nWe revisit this insensitivity, especially when sampling random demonstration\nexamples to motivate the need for example selection.\nHere is a table of the performance of various GPT-2 and GPT-31\nmodels on 4 tasks, after applying calibration (Zhao et al. 2021):\nwe randomly sample 5 set of 4-shot demonstration examples, reporting the average\nperformance and the standard deviation (in parentheses).\nModel\nAGNews (\\(\\sigma\\))\nAmazon (\\(\\sigma\\))\nSST-2 (\\(\\sigma\\))\nTREC (\\(\\sigma\\))\nGPT-2 (345M)\n55.2 (12.0)\n76.3 (14.0)\n66.2 (14.7)\n40.8 (5.4)\nGPT-3 (Ada)\n64.0 (4.0)\n90.0 (1.2)\n73.8 (9.7)\n22.1 (5.3)\nGPT-3 (Babbage)\n78.1 (6.1)\n92.7 (1.6)\n90.8 (1.1)\n36.0 (4.0)\nWhile achieving good performance on Amazon and SST-2, we observe that the\nperformance of in-context learning is volatile:\nGPT-2 demonstrates double-digit standard deviation across datasets even with\ncalibration.\nWhile the variance diminishes for larger models on sentiment classification tasks\n(Amazon and SST-2), they are still quite big for the other tasks.\nTo address this sensitivity, we propose a framework for explicitly learning\npolicies to select good examples.\nLearning to Select Examples\nThe Framework\nAs mentioned previously, we consider the problem of selecting demonstration\nexamples from an unlabeled pool.\nThe challenge with selecting a sequence of demonstration examples is there are\ntoo many candidate sequences to consider: the number of potential sequences\ngrow exponentially with the size of the unlabeled pool, and is intractable\nto enumerate.\nOne solution to this challenge is to consider example selection as a sequential\ndecision problem, selecting examples one-by-one to construct the prompt.\nIn the language of a Markov Decision Process, states in the example selection\nenvironment are examples that are already selected as part of the prompt,\nand actions represent potential examples to be selected.\nReward Function\nTo train an example selection policy, we need signals for rewarding the model\nto select good examples.\nSuppose there is an objective function \\(f : \\mathcal{X}^\\star \\to \\mathbb{R}\\)\nthat measures how good a sequence of examples is (in our implementation,\n\\(f\\) measures the performance of the prompt on a validation set).\nThen, to select a sequence of \\(k\\) examples, the trivial reward function rewards\n\\(f\\) on a complete prompt:\n\\[\nr(x_1, x_2, \\dots , x_i) =\n\\begin{cases}\n    f(x_1, x_2, \\dots, x_i) &\\text{if $i = k$} \\\\\n    0 &\\text{otherwise.}\n\\end{cases}\n\\]\nWhile this reward directly maximizes the objective \\(f\\), it does provide reward\nsignals to the example selected at time steps \\(i < t\\).\nOne way to get around this issue is reward shaping (Ng, Harada, and Russell 1999),\na modification of the reward function that preserves optimal policies.\nThe following reward function \\(r'\\) is a shaped version of \\(r\\)2\n\\[\nr'(x_1, x_2, \\dots , x_i) =\n\\begin{cases}\n    f(x_1) - f(\\varnothing) &\\text{if $i = 1$} \\\\\n    f(x_1, x_2, \\dots, x_i) - f(x_1, x_2, \\dots, x_{i - 1}) &\\text{if $i > 1$,}\n\\end{cases}\n\\]\nwhere \\(f(\\varnothing)\\) is the performance of an empty prompt.\nThe shaped reward \\(r'\\) has an intuitive interpretation: it represents the\nmarginal utility (i.e. gain over the objective \\(f\\)) of the added example.\nExperiments\nWe experiment with 4-shot example selection on GPT-2 (345M)\nand consider three baselines for comparison.\nrandom: Randomly sample demonstration examples.\nmax-entropy: Iteratively select the example that the model is least\nconfident in its prediction (measured by the entropy of the\npredicted probability distribution).\nreordering: Randomly sample demonstration examples, and apply the Global\nEntropy reordering heuristic by (Lu et al. 2022).\nResults\nDuring training, we use a training pool from which the policy learns to\nselect examples, and a reward set on which we compute reward for training\nthe policy.\nSince the policy gets direct reward signal (validation performance) on the\ntraining pool, performance on this setting (same task, seen examples)\nserve mostly as a sanity check.\nWe evaluate the generalization of learned example selection policies in two settings:\n- same task, new examples: during evaluation, the policy picks from new examples\nunder the same distribution of training.\n- new task, new examples: the policy is jointly optimized on three out of the\nfour tasks, and evaluated by selecting examples for the heldout task.\nMethod\nAverage\nAGNews (95% CI)\nAmazon (95% CI)\nSST-2 (95% CI)\nTREC (95% CI)\nrandom\n59.6\n55.2 (10.5)\n76.3 (12.3)\n66.2 (12.9)\n40.8 (4.7)\nmax-entropy\n59.3\n58.8 (11.3)\n74.8 (5.1)\n65.7 (10.7)\n37.8 (6.7)\nreordering\n63.5\n63.3 (6.8)\n89.8 (3.8)\n67.9 (11.1)\n33.0 (4.2)\nour method (same task, seen examples)\n71.4\n70.8 (7.8)\n90.4 (1.9)\n81.0 (3.5)\n43.3 (2.0)\nour method (same task, new examples)\n69.0\n65.5 (7.4)\n88.5 (4.2)\n76.7 (7.5)\n45.4 (5.0)\nour method (new task, new examples)\n65.4\n66.7 (5.7)\n89.9 (1.6)\n61.9 (7.7)\n43.3 (4.4)\nOn seen examples, the example selection outperforms the random\nsampling baseline by 11.8%, indicating learnability of the example selection\nproblem.\nPerhaps more interestingly, the learned example selection policy can generalize\nbeyond in both same task, new examples and new task, new examples,\nshowing an the best baseline (reordering) by 5.5% and 1.9% respectively.\nConcluding Discussion\nWhile our main experiments are done with GPT-2 (334M), we experimented with\ntransferring both the learned policy and selected examples to GPT-3 models.\nThe results are mixed: we observe small gains for GPT-3 Ada, but limited or\nnegative results for larger models (GPT-3 Babbage and Curie).\nThis observation might point to emergence: larger models have different\npreferences for demonstration examples.\nWhile example selection can be framed as an learning problem, it may not always\nbe the sensible option under practical considerations.\nTraining example selection policies come with significant computational\noverhead, and this cost seems hard to justify when the simple best-of-\\(k\\)\nsampling strategy can achieve great performance with a moderately sized\nvalidation set.\n\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” In Advances in Neural Information Processing Systems, 33:1877–1901. Curran Associates, Inc.\n\n\nLiu, Jiachang, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. “What Makes Good In-Context Examples for GPT-3?” In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 100–114. Dublin, Ireland and Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.deelio-1.10.\n\n\nLu, Yao, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.” In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 8086–98. Dublin, Ireland: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.556.\n\n\nMishra, Swaroop, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022. “Reframing Instructional Prompts to GPTk’s Language.” In Findings of the Association for Computational Linguistics: ACL 2022, 589–612. Dublin, Ireland: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.findings-acl.50.\n\n\nNg, Andrew Y., Daishi Harada, and Stuart J. Russell. 1999. “Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping.” In Proceedings of the Sixteenth International Conference on Machine Learning, 278–87. ICML ’99. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nRubin, Ohad, Jonathan Herzig, and Jonathan Berant. 2022. “Learning To Retrieve Prompts for In-Context Learning.” In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2655–71. Seattle, United States: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.191.\n\n\nZhao, Zihao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. “Calibrate Before Use: Improving Few-shot Performance of Language Models.” In Proceedings of the 38th International Conference on Machine Learning, 12697–706. PMLR.\n\n\nWe use text-ada-001 and text-babbage-001 in our experiments.↩︎\nRequires \\(\\gamma = 1\\).↩︎\n",
      "last_modified": "2022-12-08T09:46:19-06:00"
    },
    {
      "path": "index.html",
      "title": "Active Example Selection for In-Context Learning",
      "author": [
        {
          "name": {},
          "url": "https://y0mingzhang.github.io/"
        },
        {
          "name": {},
          "url": "https://ihsgnef.github.io/"
        },
        {
          "name": {},
          "url": "https://chenhaot.com/"
        }
      ],
      "date": "December 5, 2022",
      "contents": "\n\nContents\nIntroduction\nSensitivity to the Choice of Examples\nLearning to Select Examples\nThe Framework\nReward Function\n\nExperiments\nResults\n\nConcluding Discussion\n\nIntroduction\nLarge language models, such as GPT-3 (Brown et al. 2020) demonstrate\nan emergent capability, known as in-context learning, to perform a task by\nsimply observing information (such as instructions and demonstration examples)\nin its prompt. Despite its incredible success on many tasks, in-context learning\nperformance very much depends on a good prompt (Mishra et al. 2022).\nIn this work, we approach prompting from the perspective of example selection.\nThat is, we seek to answer: how to select good examples for in-context learning?\nDifferent from prior work which retrieves examples assuming access to individual\ntest instances (Liu et al. 2022; Rubin, Herzig, and Berant 2022), we\naim to select good examples for the entire testing distribution.\nSensitivity to the Choice of Examples\nSensitivity of in-context learning to changes in the prompt is identified by\nprior work (Zhao et al. 2021; Lu et al. 2022).\nWe revisit this insensitivity, especially when sampling random demonstration\nexamples to motivate the need for example selection.\nHere is a table of the performance of various GPT-2 and GPT-31\nmodels on 4 tasks, after applying calibration (Zhao et al. 2021):\nwe randomly sample 5 set of 4-shot demonstration examples, reporting the average\nperformance and the standard deviation (in parentheses).\nModel\nAGNews (\\(\\sigma\\))\nAmazon (\\(\\sigma\\))\nSST-2 (\\(\\sigma\\))\nTREC (\\(\\sigma\\))\nGPT-2 (345M)\n55.2 (12.0)\n76.3 (14.0)\n66.2 (14.7)\n40.8 (5.4)\nGPT-3 (Ada)\n64.0 (4.0)\n90.0 (1.2)\n73.8 (9.7)\n22.1 (5.3)\nGPT-3 (Babbage)\n78.1 (6.1)\n92.7 (1.6)\n90.8 (1.1)\n36.0 (4.0)\nWhile achieving good performance on Amazon and SST-2, we observe that the\nperformance of in-context learning is volatile:\nGPT-2 demonstrates double-digit standard deviation across datasets even with\ncalibration.\nWhile the variance diminishes for larger models on sentiment classification tasks\n(Amazon and SST-2), they are still quite big for the other tasks.\nTo address this sensitivity, we propose a framework for explicitly learning\npolicies to select good examples.\nLearning to Select Examples\nThe Framework\nAs mentioned previously, we consider the problem of selecting demonstration\nexamples from an unlabeled pool.\nThe challenge with selecting a sequence of demonstration examples is there are\ntoo many candidate sequences to consider: the number of potential sequences\ngrow exponentially with the size of the unlabeled pool, and is intractable\nto enumerate.\nOne solution to this challenge is to consider example selection as a sequential\ndecision problem, selecting examples one-by-one to construct the prompt.\nIn the language of a Markov Decision Process, states in the example selection\nenvironment are examples that are already selected as part of the prompt,\nand actions represent potential examples to be selected.\nReward Function\nTo train an example selection policy, we need signals for rewarding the model\nto select good examples.\nSuppose there is an objective function \\(f : \\mathcal{X}^\\star \\to \\mathbb{R}\\)\nthat measures how good a sequence of examples is (in our implementation,\n\\(f\\) measures the performance of the prompt on a validation set).\nThen, to select a sequence of \\(k\\) examples, the trivial reward function rewards\n\\(f\\) on a complete prompt:\n\\[\nr(x_1, x_2, \\dots , x_i) =\n\\begin{cases}\n    f(x_1, x_2, \\dots, x_i) &\\text{if $i = k$} \\\\\n    0 &\\text{otherwise.}\n\\end{cases}\n\\]\nWhile this reward directly maximizes the objective \\(f\\), it does provide reward\nsignals to the example selected at time steps \\(i < t\\).\nOne way to get around this issue is reward shaping (Ng, Harada, and Russell 1999),\na modification of the reward function that preserves optimal policies.\nThe following reward function \\(r'\\) is a shaped version of \\(r\\)2\n\\[\nr'(x_1, x_2, \\dots , x_i) =\n\\begin{cases}\n    f(x_1) - f(\\varnothing) &\\text{if $i = 1$} \\\\\n    f(x_1, x_2, \\dots, x_i) - f(x_1, x_2, \\dots, x_{i - 1}) &\\text{if $i > 1$,}\n\\end{cases}\n\\]\nwhere \\(f(\\varnothing)\\) is the performance of an empty prompt.\nThe shaped reward \\(r'\\) has an intuitive interpretation: it represents the\nmarginal utility (i.e. gain over the objective \\(f\\)) of the added example.\nExperiments\nWe experiment with 4-shot example selection on GPT-2 (345M)\nand consider three baselines for comparison.\nrandom: Randomly sample demonstration examples.\nmax-entropy: Iteratively select the example that the model is least\nconfident in its prediction (measured by the entropy of the\npredicted probability distribution).\nreordering: Randomly sample demonstration examples, and apply the Global\nEntropy reordering heuristic by (Lu et al. 2022).\nResults\nDuring training, we use a training pool from which the policy learns to\nselect examples, and a reward set on which we compute reward for training\nthe policy.\nSince the policy gets direct reward signal (validation performance) on the\ntraining pool, performance on this setting (same task, seen examples)\nserve mostly as a sanity check.\nWe evaluate the generalization of learned example selection policies in two settings:\n- same task, new examples: during evaluation, the policy picks from new examples\nunder the same distribution of training.\n- new task, new examples: the policy is jointly optimized on three out of the\nfour tasks, and evaluated by selecting examples for the heldout task.\nMethod\nAverage\nAGNews (95% CI)\nAmazon (95% CI)\nSST-2 (95% CI)\nTREC (95% CI)\nrandom\n59.6\n55.2 (10.5)\n76.3 (12.3)\n66.2 (12.9)\n40.8 (4.7)\nmax-entropy\n59.3\n58.8 (11.3)\n74.8 (5.1)\n65.7 (10.7)\n37.8 (6.7)\nreordering\n63.5\n63.3 (6.8)\n89.8 (3.8)\n67.9 (11.1)\n33.0 (4.2)\nour method (same task, seen examples)\n71.4\n70.8 (7.8)\n90.4 (1.9)\n81.0 (3.5)\n43.3 (2.0)\nour method (same task, new examples)\n69.0\n65.5 (7.4)\n88.5 (4.2)\n76.7 (7.5)\n45.4 (5.0)\nour method (new task, new examples)\n65.4\n66.7 (5.7)\n89.9 (1.6)\n61.9 (7.7)\n43.3 (4.4)\nOn seen examples, the example selection outperforms the random\nsampling baseline by 11.8%, indicating learnability of the example selection\nproblem.\nPerhaps more interestingly, the learned example selection policy can generalize\nbeyond in both same task, new examples and new task, new examples,\nshowing an the best baseline (reordering) by 5.5% and 1.9% respectively.\nConcluding Discussion\nWhile our main experiments are done with GPT-2 (334M), we experimented with\ntransferring both the learned policy and selected examples to GPT-3 models.\nThe results are mixed: we observe small gains for GPT-3 Ada, but limited or\nnegative results for larger models (GPT-3 Babbage and Curie).\nThis observation might point to emergence: larger models have different\npreferences for demonstration examples.\nWhile example selection can be framed as an learning problem, it may not always\nbe the sensible option under practical considerations.\nTraining example selection policies come with significant computational\noverhead, and this cost seems hard to justify when the simple best-of-\\(k\\)\nsampling strategy can achieve great performance with a moderately sized\nvalidation set.\n\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” In Advances in Neural Information Processing Systems, 33:1877–1901. Curran Associates, Inc.\n\n\nLiu, Jiachang, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. “What Makes Good In-Context Examples for GPT-3?” In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 100–114. Dublin, Ireland and Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.deelio-1.10.\n\n\nLu, Yao, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.” In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 8086–98. Dublin, Ireland: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.acl-long.556.\n\n\nMishra, Swaroop, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022. “Reframing Instructional Prompts to GPTk’s Language.” In Findings of the Association for Computational Linguistics: ACL 2022, 589–612. Dublin, Ireland: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.findings-acl.50.\n\n\nNg, Andrew Y., Daishi Harada, and Stuart J. Russell. 1999. “Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping.” In Proceedings of the Sixteenth International Conference on Machine Learning, 278–87. ICML ’99. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nRubin, Ohad, Jonathan Herzig, and Jonathan Berant. 2022. “Learning To Retrieve Prompts for In-Context Learning.” In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2655–71. Seattle, United States: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.191.\n\n\nZhao, Zihao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. “Calibrate Before Use: Improving Few-shot Performance of Language Models.” In Proceedings of the 38th International Conference on Machine Learning, 12697–706. PMLR.\n\n\nWe use text-ada-001 and text-babbage-001 in our experiments.↩︎\nRequires \\(\\gamma = 1\\).↩︎\n",
      "last_modified": "2022-12-08T09:46:19-06:00"
    }
  ],
  "collections": []
}
