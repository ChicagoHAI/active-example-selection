@article{arulkumaranBriefSurveyDeep2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {6},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiming/Zotero/storage/PDPFIKJR/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf;/Users/yiming/Zotero/storage/24QTELZT/1708.html}
}

@inproceedings{bakkerReinforcementLearningLong2001,
  title = {Reinforcement {{Learning}} with {{Long Short-Term Memory}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bakker, Bram},
  year = {2001},
  volume = {14},
  publisher = {{MIT Press}},
  abstract = {This paper presents reinforcement learning with a Long Short(cid:173) Term Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies be(cid:173) tween relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task.},
  file = {/Users/yiming/Zotero/storage/JXYFMNB6/Bakker - 2001 - Reinforcement Learning with Long Short-Term Memory.pdf}
}

@book{bellmanDynamicProgramming1957,
  title = {Dynamic Programming},
  author = {Bellman, Richard},
  year = {1957},
  edition = {First},
  publisher = {{Princeton University Press}},
  address = {{Princeton, NJ, USA}},
  bib2html_rescat = {General RL}
}

@article{bellmanMarkovianDecisionProcess1957,
  title = {A Markovian Decision Process},
  author = {Bellman, Richard},
  year = {1957},
  journal = {Indiana University Mathematics Journal},
  volume = {6},
  number = {4},
  pages = {679--684},
  issn = {0022-2518},
  coden = {IUMJAB},
  fjournal = {Indiana University Mathematics Journal}
}

@article{brockmanOpenAIGym2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.01540 [cs]},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/A5VBP3AJ/Brockman et al. - 2016 - OpenAI Gym.pdf;/Users/yiming/Zotero/storage/58P5HHF6/1606.html}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/Users/yiming/Zotero/storage/FME478M9/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{chamieFiniteHorizonMarkovDecision2015,
  title = {Finite-{{Horizon Markov Decision Processes}} with {{State Constraints}}},
  author = {Chamie, Mahmoud El and Acikmese, Behcet},
  year = {2015},
  month = jul,
  journal = {arXiv:1507.01585 [cs, math]},
  eprint = {1507.01585},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Markov Decision Processes (MDPs) have been used to formulate many decision-making problems in science and engineering. The objective is to synthesize the best decision (action selection) policies to maximize expected rewards (minimize costs) in a given stochastic dynamical environment. In many practical scenarios (multi-agent systems, telecommunication, queuing, etc.), the decision-making problem can have state constraints that must be satisfied, which leads to Constrained MDP (CMDP) problems. In the presence of such state constraints, the optimal policies can be very hard to characterize. This paper introduces a new approach for finding non-stationary randomized policies for finite-horizon CMDPs. An efficient algorithm based on Linear Programming (LP) and duality theory is proposed, which gives the convex set of feasible policies and ensures that the expected total reward is above a computable lower-bound. The resulting decision policy is a randomized policy, which is the projection of the unconstrained deterministic MDP policy on this convex set. To the best of our knowledge, this is the first result in state constrained MDPs to give an efficient algorithm for generating finite horizon randomized policies for CMDP with optimality guarantees. A simulation example of a swarm of autonomous agents running MDPs is also presented to demonstrate the proposed CMDP solution algorithm.},
  archiveprefix = {arXiv},
  keywords = {90C40,Electrical Engineering and Systems Science - Systems and Control,G.1.6,G.3,I.2.8,Mathematics - Optimization and Control},
  file = {/Users/yiming/Zotero/storage/UDJKUT5X/Chamie and Acikmese - 2015 - Finite-Horizon Markov Decision Processes with Stat.pdf;/Users/yiming/Zotero/storage/Z7FHU9IN/1507.html}
}

@inproceedings{daganCommitteebasedSamplingTraining1995,
  title = {Committee-Based Sampling for Training Probabilistic Classifiers},
  booktitle = {Proceedings of the Twelfth International Conference on International Conference on Machine Learning},
  author = {Dagan, Ido and Engelson, Sean P.},
  year = {1995},
  series = {{{ICML}}'95},
  pages = {150--157},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  isbn = {1-55860-377-8}
}

@misc{dengRLPromptOptimizingDiscrete2022,
  title = {{{RLPrompt}}: {{Optimizing Discrete Text Prompts With Reinforcement Learning}}},
  shorttitle = {{{RLPrompt}}},
  author = {Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P. and Hu, Zhiting},
  year = {2022},
  month = may,
  number = {arXiv:2205.12548},
  eprint = {2205.12548},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.12548},
  abstract = {Prompting has shown impressive success in enabling large pretrained language models (LMs) to perform diverse NLP tasks, especially when only few downstream data are available. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning soft prompt (e.g., embeddings) which falls short of interpretability, reusability across LMs, and applicability when gradients are not accessible. Discrete prompt, on the other hand, is difficult to optimize, and is often created by "enumeration (e.g., paraphrasing)-then-selection" heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the desired discrete prompt after training with reward. To overcome the complexity and stochasticity of reward signals by the large LM environment, we incorporate effective reward stabilization that substantially enhances the training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing finetuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating LM prompting may not follow human language patterns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/64UFICZM/Deng et al. - 2022 - RLPrompt Optimizing Discrete Text Prompts With Re.pdf}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/Users/yiming/Zotero/storage/WWKHD4QR/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@inproceedings{fangLearningHowActive2017,
  title = {Learning How to {{Active Learn}}: {{A Deep Reinforcement Learning Approach}}},
  shorttitle = {Learning How to {{Active Learn}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Fang, Meng and Li, Yuan and Cohn, Trevor},
  year = {2017},
  month = sep,
  pages = {595--605},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1063},
  abstract = {Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.},
  file = {/Users/yiming/Zotero/storage/ACNSWKV9/Fang et al. - 2017 - Learning how to Active Learn A Deep Reinforcement.pdf}
}

@inproceedings{gaoMakingPretrainedLanguage2021,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  year = {2021},
  month = aug,
  pages = {3816--3830},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.295},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\textemdash better few-shot fine-tuning of language models\textemdash a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  file = {/Users/yiming/Zotero/storage/WSJHK2IQ/Gao et al. - 2021 - Making Pre-trained Language Models Better Few-shot.pdf}
}

@inproceedings{hasseltDoubleQlearning2010,
  title = {Double {{Q-learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hasselt, Hado},
  year = {2010},
  volume = {23},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.},
  file = {/Users/yiming/Zotero/storage/CVIDSQB6/Hasselt - 2010 - Double Q-learning.pdf}
}

@misc{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2017},
  month = oct,
  number = {arXiv:1710.02298},
  eprint = {1710.02298},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/ZTBV3WFZ/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;/Users/yiming/Zotero/storage/FILVR9GD/1710.html}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {{MIT Press}}
}

@misc{holtzmanSurfaceFormCompetition2021,
  title = {Surface {{Form Competition}}: {{Why}} the {{Highest Probability Answer Isn}}'t {{Always Right}}},
  shorttitle = {Surface {{Form Competition}}},
  author = {Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  year = {2021},
  month = sep,
  number = {arXiv:2104.08315},
  eprint = {2104.08315},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Large language models have shown promising results in zero-shot settings (Brown et al.,2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept, e.g. "computer" and "PC." Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to a term that is proportional to its a priori likelihood within the context of the specific zero-shot task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models over a variety of multiple choice datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiming/Zotero/storage/VH72GLAZ/Holtzman et al. - 2021 - Surface Form Competition Why the Highest Probabil.pdf;/Users/yiming/Zotero/storage/42Q8GSUM/2104.html}
}

@misc{kirkSurveyGeneralisationDeep2022,
  title = {A {{Survey}} of {{Generalisation}} in {{Deep Reinforcement Learning}}},
  author = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  year = {2022},
  month = jan,
  number = {arXiv:2111.09794},
  eprint = {2111.09794},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overfitting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent field. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the field, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-specific problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as offline RL generalisation and reward-function variation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/8BJIPAFW/Kirk et al. - 2022 - A Survey of Generalisation in Deep Reinforcement L.pdf;/Users/yiming/Zotero/storage/Z3LRA9EP/2111.html}
}

@misc{kojimaLargeLanguageModels2022,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2022},
  month = may,
  number = {arXiv:2205.11916},
  eprint = {2205.11916},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.11916},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding ``Let's think step by step'' before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with an off-the-shelf 175B parameter model. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/LUGCDF7L/Kojima et al. - 2022 - Large Language Models are Zero-Shot Reasoners.pdf;/Users/yiming/Zotero/storage/HACWB2JX/2205.html}
}

@misc{kumarConservativeQLearningOffline2020,
  title = {Conservative {{Q-Learning}} for {{Offline Reinforcement Learning}}},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  year = {2020},
  month = aug,
  number = {arXiv:2006.04779},
  eprint = {2006.04779},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiming/Zotero/storage/ER7LSYLQ/Kumar et al. - 2020 - Conservative Q-Learning for Offline Reinforcement .pdf;/Users/yiming/Zotero/storage/SKAS4DZ2/2006.html}
}

@article{lanESTIMATIONBIASQLEARNING2020,
  title = {{{ESTIMATION BIAS OF Q-LEARNING}}},
  author = {Lan, Qingfeng and Pan, Yangchen and Fyshe, Alona and White, Martha},
  year = {2020},
  pages = {20},
  langid = {english},
  file = {/Users/yiming/Zotero/storage/6JJM67XQ/Lan et al. - 2020 - ESTIMATION BIAS OF Q-LEARNING.pdf}
}

@article{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  author = {Lester, Brian and {Al-Rfou}, Rami and Constant, Noah},
  year = {2021},
  month = sep,
  journal = {arXiv:2104.08691 [cs]},
  eprint = {2104.08691},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiming/Zotero/storage/28YSL6DS/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf;/Users/yiming/Zotero/storage/LUAKSWLS/2104.html}
}

@article{linSelfImprovingReactiveAgents1992,
  title = {Self-{{Improving Reactive Agents Based}} on {{Reinforcement Learning}}, {{Planning}} and {{Teaching}}},
  author = {Lin, Long-Ji},
  year = {1992},
  month = may,
  journal = {Machine Language},
  volume = {8},
  number = {3-4},
  pages = {293--321},
  issn = {0885-6125},
  doi = {10.1007/BF00992699},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning. This paper compares eight reinforcement learning frameworks: adaptive heuristic critic (AHC) learning due to Sutton, Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.},
  keywords = {connectionist networks,planning,Reinforcement learning,teaching},
  file = {/Users/yiming/Zotero/storage/4PHMTANT/Lin - 1992 - Self-Improving Reactive Agents Based on Reinforcem.pdf}
}

@inproceedings{liPrefixTuningOptimizingContinuous2021,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Xiang Lisa and Liang, Percy},
  year = {2021},
  month = aug,
  pages = {4582--4597},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.353},
  abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were ``virtual tokens''. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.},
  file = {/Users/yiming/Zotero/storage/XDAITKKT/Li and Liang - 2021 - Prefix-Tuning Optimizing Continuous Prompts for G.pdf}
}

@inproceedings{liuLearningHowActively2018,
  title = {Learning {{How}} to {{Actively Learn}}: {{A Deep Imitation Learning Approach}}},
  shorttitle = {Learning {{How}} to {{Actively Learn}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Ming and Buntine, Wray and Haffari, Gholamreza},
  year = {2018},
  month = jul,
  pages = {1874--1883},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1174},
  abstract = {Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. We introduce a method that learns an AL ``policy'' using ``imitation learning'' (IL). Our IL-based approach makes use of an efficient and effective ``algorithmic expert'', which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our method on two different tasks: text classification and named entity recognition. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.},
  file = {/Users/yiming/Zotero/storage/WBPG2ZLJ/Liu et al. - 2018 - Learning How to Actively Learn A Deep Imitation L.pdf}
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiming/Zotero/storage/HBNKGHAB/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/yiming/Zotero/storage/CPJNEVVJ/1907.html}
}

@inproceedings{liuWhatMakesGood2022,
  title = {What {{Makes Good In-Context Examples}} for {{GPT-3}}?},
  booktitle = {Proceedings of {{Deep Learning Inside Out}} ({{DeeLIO}} 2022): {{The}} 3rd {{Workshop}} on {{Knowledge Extraction}} and {{Integration}} for {{Deep Learning Architectures}}},
  author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  year = {2022},
  month = may,
  pages = {100--114},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland and Online}},
  doi = {10.18653/v1/2022.deelio-1.10},
  abstract = {GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3\% on the ToTTo dataset) and open-domain question answering (45.5\% on the NQ dataset).},
  file = {/Users/yiming/Zotero/storage/YSERZDVW/Liu et al. - 2022 - What Makes Good In-Context Examples for GPT-3.pdf}
}

@inproceedings{luFantasticallyOrderedPrompts2022,
  title = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}: {{Overcoming Few-Shot Prompt Order Sensitivity}}},
  shorttitle = {Fantastically {{Ordered Prompts}} and {{Where}} to {{Find Them}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  year = {2022},
  month = may,
  pages = {8086--8098},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.556},
  abstract = {When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are ``fantastic'' and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13\% relative improvement for GPT-family models across eleven different established text classification tasks.},
  file = {/Users/yiming/Zotero/storage/JK73JKMW/Lu et al. - 2022 - Fantastically Ordered Prompts and Where to Find Th.pdf}
}

@article{minRethinkingRoleDemonstrations2022,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.12837 [cs]},
  eprint = {2202.12837},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiming/Zotero/storage/8JM3HEI6/Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes .pdf;/Users/yiming/Zotero/storage/GFAQZWSI/2202.html}
}

@inproceedings{mishraReframingInstructionalPrompts2022,
  title = {Reframing {{Instructional Prompts}} to {{GPTk}}'s {{Language}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Choi, Yejin and Hajishirzi, Hannaneh},
  year = {2022},
  month = may,
  pages = {589--612},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.50},
  abstract = {What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5\% and 6.7\% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.},
  file = {/Users/yiming/Zotero/storage/CLIMRCB5/Mishra et al. - 2022 - Reframing Instructional Prompts to GPTk's Language.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science},
  file = {/Users/yiming/Zotero/storage/64UANFFI/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf;/Users/yiming/Zotero/storage/QHP8C4PI/nature14236.html}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.5602},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/BYT4FEMX/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/Users/yiming/Zotero/storage/APKW95I5/1312.html}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  year = {2022},
  month = jun,
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2112.09332},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/VNIXVIEP/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/Users/yiming/Zotero/storage/N9WDQ5ZL/2112.html}
}

@inproceedings{ngPolicyInvarianceReward1999,
  title = {Policy {{Invariance Under Reward Transformations}}: {{Theory}} and {{Application}} to {{Reward Shaping}}},
  shorttitle = {Policy {{Invariance Under Reward Transformations}}},
  booktitle = {Proceedings of the {{Sixteenth International Conference}} on {{Machine Learning}}},
  author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
  year = {1999},
  month = jun,
  series = {{{ICML}} '99},
  pages = {278--287},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  isbn = {978-1-55860-612-8}
}

@inproceedings{pathakCuriosityDrivenExplorationSelfSupervised2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self-Supervised Prediction}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {488--489},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.70},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  isbn = {978-1-5386-0733-6},
  langid = {english},
  file = {/Users/yiming/Zotero/storage/K3NDR7CM/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf}
}

@inproceedings{perezTrueFewShotLearning2021,
  title = {True {{Few-Shot Learning}} with {{Language Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  year = {2021},
  month = may,
  abstract = {Language models do much worse at few-shot learning when choosing prompts and hyperparameters in a few-shot way, rather than by using many held-out examples as in prior work.},
  langid = {english},
  file = {/Users/yiming/Zotero/storage/VWDDKPGW/Perez et al. - 2021 - True Few-Shot Learning with Language Models.pdf;/Users/yiming/Zotero/storage/Z5SJM56V/forum.html}
}

@misc{prudencioSurveyOfflineReinforcement2022,
  title = {A {{Survey}} on {{Offline Reinforcement Learning}}: {{Taxonomy}}, {{Review}}, and {{Open Problems}}},
  shorttitle = {A {{Survey}} on {{Offline Reinforcement Learning}}},
  author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01387},
  eprint = {2203.01387},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications such as education, healthcare, and robotics. In this work, we propose a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field, and a review of existing benchmarks' properties and shortcomings. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiming/Zotero/storage/R2K2KL4M/Prudencio et al. - 2022 - A Survey on Offline Reinforcement Learning Taxono.pdf;/Users/yiming/Zotero/storage/83JL55L5/2203.html}
}

@article{radford2019language,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019}
}

@misc{raeScalingLanguageModels2022,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and {d'Autume}, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  year = {2022},
  month = jan,
  number = {arXiv:2112.11446},
  eprint = {2112.11446},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiming/Zotero/storage/2RWLRCLW/Rae et al. - 2022 - Scaling Language Models Methods, Analysis & Insig.pdf;/Users/yiming/Zotero/storage/86A29PKT/2112.html}
}

@misc{raffelExploringLimitsTransfer2020a,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiming/Zotero/storage/JX88CPHS/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;/Users/yiming/Zotero/storage/U4VMF5AH/1910.html}
}

@inproceedings{rubinLearningRetrievePrompts2022,
  title = {Learning {{To Retrieve Prompts}} for {{In-Context Learning}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  year = {2022},
  month = jul,
  pages = {2655--2671},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.191},
  abstract = {In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.},
  file = {/Users/yiming/Zotero/storage/8BA6I4VS/Rubin et al. - 2022 - Learning To Retrieve Prompts for In-Context Learni.pdf}
}

@article{settlesActiveLearningLiterature2009,
  title = {Active Learning Literature Survey},
  author = {Settles, Burr},
  year = {2009},
  publisher = {{University of Wisconsin-Madison Department of Computer Sciences}}
}

@article{sharafMetaLearningContextualBandit2019,
  title = {Meta-{{Learning}} for {{Contextual Bandit Exploration}}},
  author = {Sharaf, Amr and Daum{\'e} III, Hal},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.08159 [cs, stat]},
  eprint = {1901.08159},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We describe MELEE, a meta-learning algorithm for learning a good exploration policy in the interactive contextual bandit setting. Here, an algorithm must take actions based on contexts, and learn based only on a reward signal from the action taken, thereby generating an exploration/exploitation trade-off. MELEE addresses this trade-off by learning a good exploration strategy for offline tasks based on synthetic data, on which it can simulate the contextual bandit setting. Based on these simulations, MELEE uses an imitation learning strategy to learn a good exploration policy that can then be applied to true contextual bandit tasks at test time. We compare MELEE to seven strong baseline contextual bandit algorithms on a set of three hundred real-world datasets, on which it outperforms alternatives in most settings, especially when differences in rewards are large. Finally, we demonstrate the importance of having a rich feature representation for learning how to explore.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiming/Zotero/storage/49VTIX3K/Sharaf and Daumé III - 2019 - Meta-Learning for Contextual Bandit Exploration.pdf;/Users/yiming/Zotero/storage/FBZ3DSM5/1901.html}
}

@inproceedings{socherRecursiveDeepModels2013,
  title = {Recursive {{Deep Models}} for {{Semantic Compositionality Over}} a {{Sentiment Treebank}}},
  booktitle = {Proceedings of the 2013 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
  year = {2013},
  month = oct,
  pages = {1631--1642},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, Washington, USA}},
  file = {/Users/yiming/Zotero/storage/BJEIVVG9/Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionalit.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: {{An}} Introduction},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  publisher = {{A Bradford Book}},
  address = {{Cambridge, MA, USA}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  isbn = {0-262-03924-9}
}

@book{von1893natural,
  title = {Natural Value},
  author = {Von Wieser, Friedrich Freiherr},
  year = {1893},
  publisher = {{Macmillan and Company}}
}

@inproceedings{voorheesBuildingQuestionAnswering2000,
  title = {Building a Question Answering Test Collection},
  booktitle = {Proceedings of the 23rd Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Voorhees, Ellen M. and Tice, Dawn M.},
  year = {2000},
  month = jul,
  series = {{{SIGIR}} '00},
  pages = {200--207},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/345508.345577},
  abstract = {The TREC-8 Question Answering (QA) Track was the first large-scale evaluation of domain-independent question answering systems. In addition to fostering research on the QA task, the track was used to investigate whether the evaluation methodology used for document retrieval is appropriate for a different natural language processing task. As with document relevance judging, assessors had legitimate differences of opinions as to whether a response actually answers a question, but comparative evaluation of QA systems was stable despite these differences. Creating a reusable QA test collection is fundamentally more difficult than creating a document retrieval test collection since the QA task has no equivalent to document identifiers.},
  isbn = {978-1-58113-226-7},
  file = {/Users/yiming/Zotero/storage/AYKGTEK3/Voorhees and Tice - 2000 - Building a question answering test collection.pdf}
}

@article{vuSPoTBetterFrozen2022,
  title = {{{SPoT}}: {{Better Frozen Model Adaptation}} through {{Soft Prompt Transfer}}},
  shorttitle = {{{SPoT}}},
  author = {Vu, Tu and Lester, Brian and Constant, Noah and {Al-Rfou}, Rami and Cer, Daniel},
  year = {2022},
  month = mar,
  journal = {arXiv:2110.07904 [cs]},
  eprint = {2110.07904},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {There has been growing interest in parameter-efficient methods to apply pre-trained language models to downstream tasks. Building on the Prompt Tuning approach of Lester et al. (2021), which learns task-specific soft prompts to condition a frozen pre-trained model to perform different tasks, we propose a novel prompt-based transfer learning approach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one or more source tasks and then uses it to initialize the prompt for a target task. We show that SPoT significantly boosts the performance of Prompt Tuning across many tasks. More remarkably, across all model sizes, SPoT matches or outperforms standard Model Tuning (which fine-tunes all model parameters) on the SuperGLUE benchmark, while using up to 27,000x fewer task-specific parameters. To understand where SPoT is most effective, we conduct a large-scale study on task transferability with 26 NLP tasks in 160 combinations, and demonstrate that many tasks can benefit each other via prompt transfer. Finally, we propose an efficient retrieval approach that interprets task prompts as task embeddings to identify similar tasks and predict the most transferable source tasks for a novel target task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiming/Zotero/storage/N72SUMDJ/Vu et al. - 2022 - SPoT Better Frozen Model Adaptation through Soft .pdf;/Users/yiming/Zotero/storage/A2D2B8PD/2110.html}
}

@inproceedings{wangSuperGLUEStickierBenchmark2019,
  title = {{{SuperGLUE}}: {{A Stickier Benchmark}} for {{General-Purpose Language Understanding Systems}}},
  shorttitle = {{{SuperGLUE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at https://super.gluebenchmark.com.},
  file = {/Users/yiming/Zotero/storage/G28TIBGJ/Wang et al. - 2019 - SuperGLUE A Stickier Benchmark for General-Purpos.pdf}
}

@incollection{wierstraSolvingDeepMemory2007,
  title = {Solving {{Deep Memory POMDPs}} with {{Recurrent Policy Gradients}}},
  booktitle = {Artificial {{Neural Networks}} \textendash{} {{ICANN}} 2007},
  author = {Wierstra, Daan and Foerster, Alexander and Peters, Jan and Schmidhuber, J{\"u}rgen},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and {de S{\'a}}, Joaquim Marques and Alexandre, Lu{\'i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
  year = {2007},
  volume = {4668},
  pages = {697--706},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-74690-4_71},
  abstract = {This paper presents Recurrent Policy Gradients, a modelfree reinforcement learning (RL) method creating limited-memory stochastic policies for partially observable Markov decision problems (POMDPs) that require long-term memories of past observations. The approach involves approximating a policy gradient for a Recurrent Neural Network (RNN) by backpropagating return-weighted characteristic eligibilities through time. Using a ``Long Short-Term Memory'' architecture, we are able to outperform other RL methods on two important benchmark tasks. Furthermore, we show promising results on a complex car driving simulation task.},
  isbn = {978-3-540-74689-8 978-3-540-74690-4},
  langid = {english},
  file = {/Users/yiming/Zotero/storage/64XIZGAL/Wierstra et al. - 2007 - Solving Deep Memory POMDPs with Recurrent Policy G.pdf}
}

@article{woodwardActiveOneshotLearning2017,
  title = {Active {{One-shot Learning}}},
  author = {Woodward, Mark and Finn, Chelsea},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.06559 [cs]},
  eprint = {1702.06559},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/U8GBLSKY/Woodward and Finn - 2017 - Active One-shot Learning.pdf;/Users/yiming/Zotero/storage/J78RZ5TA/1702.html}
}

@article{wuIDPGInstanceDependentPrompt2022,
  title = {{{IDPG}}: {{An Instance-Dependent Prompt Generation Method}}},
  shorttitle = {{{IDPG}}},
  author = {Wu, Zhuofeng and Wang, Sinong and Gu, Jiatao and Hou, Rui and Dong, Yuxiao and Vydiswaran, V. G. Vinod and Ma, Hao},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.04497 [cs]},
  eprint = {2204.04497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/GJEQDGJW/Wu et al. - 2022 - IDPG An Instance-Dependent Prompt Generation Meth.pdf;/Users/yiming/Zotero/storage/N537T6ZY/2204.html}
}

@misc{xieExplanationIncontextLearning2022,
  title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  year = {2022},
  month = may,
  number = {arXiv:2111.02080},
  eprint = {2111.02080},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/B2VJJ3R2/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit .pdf;/Users/yiming/Zotero/storage/IIYM7INB/2111.html}
}

@inproceedings{zhangCharacterlevelConvolutionalNetworks2015,
  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and derp learning models such as word-based ConvNets and recurrent neural networks.},
  file = {/Users/yiming/Zotero/storage/L9B3HWN2/Zhang et al. - 2015 - Character-level Convolutional Networks for Text Cl.pdf}
}

@misc{zhangOPTOpenPretrained2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = may,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.01068},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiming/Zotero/storage/HAQTEJ97/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf;/Users/yiming/Zotero/storage/PRJ4FEL9/2205.html}
}

@inproceedings{zhaoCalibrateUseImproving2021,
  title = {Calibrate {{Before Use}}: {{Improving Few-shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  year = {2021},
  month = jul,
  pages = {12697--12706},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's accuracy (up to 30.0\% absolute) across different choices of the prompt, while also making learning considerably more stable.},
  langid = {english},
  file = {/Users/yiming/Zotero/storage/J9777Y9T/Zhao et al. - 2021 - Calibrate Before Use Improving Few-shot Performan.pdf;/Users/yiming/Zotero/storage/WGXE2WMD/Zhao et al. - 2021 - Calibrate Before Use Improving Few-shot Performan.pdf}
}
